#!/usr/bin/env python3
# -*- mode: python; indent-tabs-mode: nil; python-indent-level: 4 -*-
# vim: autoindent tabstop=4 shiftwidth=4 expandtab softtabstop=4 filetype=python

import sys
import os
import lzma
from pathlib import Path
from multiprocessing import Process
TOOLBOX_HOME = os.environ.get('TOOLBOX_HOME')
if TOOLBOX_HOME is None:
    print("This script requires libraries that are provided by the toolbox project.")
    print("Toolbox can be acquired from https://github.com/perftool-incubator/toolbox and")
    print("then use 'export TOOLBOX_HOME=/path/to/toolbox' so that it can be located.")
    exit(1)
else:
    p = Path(TOOLBOX_HOME) / 'python'
    if not p.exists() or not p.is_dir():
        print("ERROR: <TOOLBOX_HOME>/python ('%s') does not exist!" % (p))
        exit(2)
    sys.path.append(str(p))
from toolbox.metrics import log_sample
from toolbox.metrics import finish_samples

def conntrack_post_process(filename: str):
    print('ovs-post-process-conntrack')
    # Made for the file conntrack-stats-show.txt.xz
    file_name = filename
    with lzma.open(file_name, 'rt') as file:
        desc = {'source': 'ovs', 'type': 'conntrack', 'class': 'count'}
        proto_tcp = {'protocol': 'tcp'}
        proto_udp = {'protocol': 'udp'}
        proto_icmp = {'protocol': 'icmp'}
        proto_other = {'protocol': 'other'}
        tcp_sample = {'end': 0, 'value': 0}
        udp_sample = {'end': 0, 'value': 0}
        icmp_sample = {'end': 0, 'value': 0}
        other_sample = {'end': 0, 'value': 0}
        file_id_ovs = 'ovs-conntrack'
        for line in file:
            if 'DATE' in line:
                # The date stamp indicates a new sample to log
                # DATE:1620741229.909499801
                time_stamp_float = float(line.split(':')[1])
                time_stamp = int(time_stamp_float * 1000)
                tcp_sample['end'] = time_stamp
                udp_sample['end'] = time_stamp
                icmp_sample['end'] = time_stamp
                other_sample['end'] = time_stamp
            elif 'TCP' in line:
                # TCP: 2587
                tcp_sample['value'] = int(line.split(':')[1])
                log_sample(file_id_ovs, desc, proto_tcp, tcp_sample)
            elif 'UDP' in line:
                # UDP: 74
                udp_sample['value'] = int(line.split(':')[1])
                log_sample(file_id_ovs, desc, proto_udp, udp_sample)
            elif 'ICMP' in line:
                # ICMP: 3
                icmp_sample['value'] = int(line.split(':')[1])
                log_sample(file_id_ovs, desc, proto_icmp, icmp_sample)
            elif 'Other' in line:
                # Other: 2
                other_sample['value'] = int(line.split(':')[1])
                log_sample(file_id_ovs, desc, proto_other, other_sample)
            else:
                continue

    finish_samples()

def dpctl_memory_show_process(filename: str):
    print('ovs-post-process-dpctl-memory-show')
    file_name = filename
    with lzma.open(file_name,'rt') as file:
        desc = {'source': 'ovs', 'class': 'count', 'type': 'dpctl-mem'}
        handler = {'source' : 'handlers'}
        ofconns = {'source' : 'ofconns'}
        ports = {'source' : 'ports'}
        revalidators = {'source' : 'revalidators'}
        rules = {'source' : 'rules'}
        udpif_keys = {'source' : 'udpif-keys'}
        handler_smpl = {'end': 0, 'value': 0}
        ofconns_smpl = {'end': 0, 'value': 0}
        ports_smpl = {'end': 0, 'value': 0}
        revalidators_smpl = {'end': 0, 'value': 0}
        rules_smpl = {'end': 0, 'value': 0}
        udpif_smpl = {'end': 0, 'value': 0}
        file_id = 'ovs-memory-show'
        for line in file:
            if 'DATE' in line:
                time_stamp_float = float(line.split(':')[1])
                time_stamp = int(time_stamp_float * 1000)
                handler_smpl['end'] = time_stamp
                ofconns_smpl['end'] = time_stamp
                ports_smpl['end'] = time_stamp
                revalidators_smpl['end'] = time_stamp
                rules_smpl['end'] = time_stamp
                udpif_smpl['end'] = time_stamp
            elif 'handlers'in line:
                # handlers:59 ofconns:2 ports:49 revalidators:21 rules:6664 udpif keys:643
                data = line.split(' ')
                try:
                    handler_smpl['value'] = int(data[0].split(':')[1])
                    ofconns_smpl['value'] = int(data[1].split(':')[1])
                    ports_smpl['value'] = int(data[2].split(':')[1])
                    revalidators_smpl['value'] = int(data[3].split(':')[1])
                    rules_smpl['value'] = int(data[4].split(':')[1])
                    udpif_smpl['value'] = int(data[7].split(':')[1])
                except IndexError as e:
                    print(e,data)
                    continue
                log_sample(file_id, desc, handler, handler_smpl)
                log_sample(file_id, desc, ofconns, ofconns_smpl)
                log_sample(file_id, desc, ports, ports_smpl)
                log_sample(file_id, desc, revalidators, revalidators_smpl)
                log_sample(file_id, desc, rules, rules_smpl)
                log_sample(file_id, desc, udpif_keys, udpif_smpl)
            else:
                continue

    finish_samples()


def compute_and_log_stats(br_name: str, new_br_data: dict, old_br_data: dict):
    # This function is responsible for calculating the rate of change, This is done with the parsed date stamp
    # rate = (new - old)/time_delta
    # example of br_data that is being parsed in this function
    '''
    {
          "timestamp": 1621349568.9893348,
          "LOCAL": {
            "rx_pkts": 2813750994,
            "rx_bytes": 1575488822034,
            "rx_drop": 0,
            "rx_errs": 0,
            "rx_frame": 0,
            "rx_over": 0,
            "rx_crc": 0,
            "tx_pkts": 2869366564,
            "tx_bytes": 1832211829840,
            "tx_drop": 0,
            "tx_errs": 0,
            "tx_coll": 0
          },
          "ens7f0": {
            "rx_pkts": 3661435731,
            "rx_bytes": 2013820599332,
            "rx_drop": 0,
            "rx_errs": 0,
            "rx_frame": 0,
            "rx_over": 0,
            "rx_crc": 0,
            "tx_pkts": 3782767418,
            "tx_bytes": 2312518145390,
            "tx_drop": 0,
            "tx_errs": 0,
            "tx_coll": 0
          },
    '''
    file_id = 'ovs-port-counters'
    log_desc = {'source': 'ovs', 'class': 'throughput'}
    if len(old_br_data.keys()) != len(new_br_data.keys()):
        print("Number of ports changed between measurements on %s" % br_name)

    time_delta = new_br_data['timestamp'] - old_br_data['timestamp']
    for intf_name in new_br_data.keys():
        if 'timestamp' in intf_name:
            continue
        for counter, val in new_br_data[intf_name].items():
            log_names = {'bridge' : br_name , 'interface': intf_name}
            # e.g. tx_bytes
            direction,name = counter.split('_')
            log_names['direction'] = direction
            if 'pkts' in name:
                log_desc['type'] = 'packets-sec'
            elif 'bytes' in name:
                log_desc['type'] = 'Gbps'
            else:
                log_desc['type'] = 'errors-sec'
                log_names['type'] = name

            try:
                rate = (val - old_br_data[intf_name][counter]) / time_delta
                # unify on Gbps for network transfer
                if log_desc['type'] == 'Gbps':
                    rate = (rate * 8) / 1_000_000_000
                sample = {'end': int(new_br_data['timestamp'] * 1000),'value': float(rate)}
                log_sample(file_id, log_desc, log_names, sample)
            except KeyError:
                # if a port appears, it will be in the new data and not the old
                # catch the key error, will only happen once per port appearance
                pass


def ofctl_port_counters(filename: str):
    print('ovs-post-process-ofctl-dump-ports')
    # assume that number of ports per bridge will change during the run
    # we don't control the coming and going of pods
    prev_bridges = {}
    bridges = {}
    curr_date = 0

    with lzma.open(filename,'rt') as file:
        curr_bridge = ''
        curr_port = ''
        for line in file:
            if 'DATE' in line:
                s = line.split(':')[1]
                curr_date = float(s)
            elif '[ovs-ofctl' in line:
                # [ovs-ofctl dump-ports br-int]\n
                br_name = line.split(' ')[-1].strip(']\n')
                curr_bridge = br_name
                # We have found a second or subsequent bridge port dump
                if br_name in bridges:
                    if br_name not in prev_bridges:
                        # alias the existing data structure with the prev_bridges data structure
                        prev_bridges[br_name] = bridges[br_name]
                    else:
                        # stats are already there, time to calc and log the stats
                        compute_and_log_stats(br_name,bridges[br_name], prev_bridges[br_name])
                        prev_bridges[br_name] = bridges[br_name]

                # init the structure with a clean dictionary
                bridges[br_name] = {}
                bridges[br_name]['timestamp'] = curr_date
            elif 'OFPST_PORT' in line:
                continue

            elif 'port' in line:
                # port_id can be a number or a string, rx_stats is the start of the port counters
                port_id,rx_stats = line.split(':')
                # parsing this string:   port "92035bf4585978e"
                port_id = port_id.split(' ')[-1].strip('"')
                curr_port = port_id
                bridges[curr_bridge][curr_port] = {}
                # rx pkts=105713661, bytes=15600717866, drop=2, errs=0, frame=0, over=0, crc=0
                rx_stats = rx_stats.split(',')
                for stat in rx_stats:
                    try:
                        key, val = stat.split('=')
                    except ValueError as e:
                        print(e,filename,line)
                    if '?' in val:
                        val = 0
                    if 'rx pkts' in key:
                        key = 'pkts'
                    key = key.strip()
                    bridges[curr_bridge][curr_port]['rx_'+ key] = int(val)

            elif 'tx pkts' in line:
                tx_stats = line.split(',')
                for stat in tx_stats:
                    key, val = stat.split('=')
                    if '?' in val:
                        val = 0
                    if 'tx pkts' in key:
                        key = 'pkts'
                    key = key.strip()
                    bridges[curr_bridge][curr_port]['tx_'+ key] = int(val)

            elif 'duration' in line:
                continue

            else:
                continue

    finish_samples()


def dpctl_datapath_stats(filename: str):
    print("ovs-post-process-dpctl-datapath-stats")
    dp_names = []
    lookups_desc_count = {'source': 'ovs', 'type': 'datapath-lookups-count', 'class': 'count'}
    masks_desc_count = {'source': 'ovs', 'type': 'datapath-masks-count', 'class': 'count'}
    cache_desc_count = {'source': 'ovs', 'type': 'datapath-cache-count', 'class': 'count'}
    flow_desc_count = {'source': 'ovs', 'type': 'datapath-flows-count', 'class': 'count'}
    lookups_desc_rate = {'source': 'ovs', 'type': 'datapath-lookups-rate', 'class': 'count'}
    masks_desc_rate = {'source': 'ovs', 'type': 'datapath-masks-rate', 'class': 'count'}
    cache_desc_rate = {'source': 'ovs', 'type': 'datapath-cache-rate', 'class': 'count'}
    flow_desc_rate = {'source': 'ovs', 'type': 'datapath-flows-rate', 'class': 'count'}
    file_id = 'dpctl-datapath-stats'
    prev_stamp = 0
    curr_stamp = 0
    time_delta = 0
    cache_hit_sample = {'end': 0, 'value': 0}
    prev_cache_hit = 0
    lookups_hit_sample = {'end': 0, 'value': 0}
    prev_lookups_hit = 0
    lookups_miss_sample = {'end': 0, 'value': 0}
    prev_lookups_miss = 0
    lookups_lost_sample = {'end': 0, 'value': 0}
    prev_lookups_lost = 0
    masks_hit_sample = {'end': 0, 'value': 0}
    prev_masks_hit = 0
    masks_total_sample = {'end': 0, 'value': 0}
    prev_masks_total = 0
    flows_sample = {'end': 0, 'value': 0}
    prev_flows = 0
    with lzma.open(filename,'rt') as file:
        for line in file:
            if 'DATE' in line:
                prev_stamp = curr_stamp
                s = float(line.split(':')[1])
                curr_stamp = int(s * 1000)
                time_delta = curr_stamp - prev_stamp
                cache_hit_sample['end'] = curr_stamp
                lookups_hit_sample['end'] = curr_stamp
                lookups_miss_sample['end'] = curr_stamp
                lookups_lost_sample['end'] = curr_stamp
                masks_hit_sample['end'] = curr_stamp
                masks_total_sample['end'] = curr_stamp
                flows_sample['end'] = curr_stamp
            elif '@' in line:
            # We have found a datapath
                name = line.split(':')[0]
                if name not in dp_names:
                    dp_names.append(name)

            elif 'lookups:' in line:
                #  lookups: hit:74624770 missed:2867652 lost:0
                pairs = line[2:].split(' ')
                curr_lookups_hit = int(pairs[1].split(':')[1])
                curr_lookups_miss = int(pairs[2].split(':')[1])
                curr_lookups_lost = int(pairs[3].split(':')[1])

                # Log the count, then the rate, both could be useful

                lookups_hit_sample['value'] = curr_lookups_hit
                lookups_miss_sample['value'] = curr_lookups_miss
                lookups_lost_sample['value'] = curr_lookups_lost

                log_sample(file_id, lookups_desc_count, {'interface': dp_names[-1], 'action': 'hit'}, lookups_hit_sample)
                log_sample(file_id, lookups_desc_count, {'interface': dp_names[-1], 'action': 'miss'}, lookups_miss_sample)
                log_sample(file_id, lookups_desc_count, {'interface': dp_names[-1], 'action': 'lost'}, lookups_lost_sample)

                lookups_hit_sample['value'] = (curr_lookups_hit - prev_lookups_hit) / time_delta
                lookups_miss_sample['value'] = (curr_lookups_miss - prev_lookups_miss) / time_delta
                lookups_lost_sample['value'] = (curr_lookups_lost - prev_lookups_lost) / time_delta
                prev_lookups_hit = curr_lookups_hit
                prev_lookups_miss = curr_lookups_miss
                prev_lookups_lost = curr_lookups_lost
                log_sample(file_id, lookups_desc_rate, {'interface': dp_names[-1], 'action': 'hit'}, lookups_hit_sample)
                log_sample(file_id, lookups_desc_rate, {'interface': dp_names[-1], 'action': 'miss'}, lookups_miss_sample)
                log_sample(file_id, lookups_desc_rate, {'interface': dp_names[-1], 'action': 'lost'}, lookups_lost_sample)

            elif 'flows:' in line:
                #  flows: 554
                pairs = line.split(':')
                curr_flows = int(pairs[1])

                # Log the count, then the rate, both could be useful

                flows_sample['value'] = curr_flows
                log_sample(file_id, flow_desc_count, {'interface': dp_names[-1], 'counter': 'flows'}, flows_sample)

                flows_sample['value'] = (curr_flows - prev_flows) / time_delta
                prev_flows = curr_flows
                log_sample(file_id, flow_desc_rate, {'interface': dp_names[-1], 'counter': 'flows'}, flows_sample)

            elif 'masks:' in line:
                #  masks: hit:512424506 total:64 hit/pkt:6.61
                pairs = line[2:].split(' ')
                curr_masks_hit = int(pairs[1].split(':')[1])
                curr_masks_total = int(pairs[2].split(':')[1])

                # Log the count, then the rate, both could be useful

                masks_hit_sample['value'] = curr_masks_hit
                masks_total_sample['value'] = curr_masks_total
                log_sample(file_id, masks_desc_count, {'interface': dp_names[-1], 'action': 'hit'}, masks_hit_sample)
                log_sample(file_id, masks_desc_count, {'interface': dp_names[-1], 'action': 'total'}, masks_total_sample)

                masks_hit_sample['value'] = (curr_masks_hit - prev_masks_hit) / time_delta
                masks_total_sample['value'] = (curr_masks_total - prev_masks_total) / time_delta
                prev_masks_hit = curr_masks_hit
                prev_masks_total = curr_masks_total
                log_sample(file_id, masks_desc_rate, {'interface': dp_names[-1], 'action': 'hit'}, masks_hit_sample)
                log_sample(file_id, masks_desc_rate, {'interface': dp_names[-1], 'action': 'total'}, masks_total_sample)

            # startswith is slower, but coding around the false positives is uglier
            elif line.startswith('  cache:'):
                #  cache: hit:46193519 hit-rate:59.61%
                pairs = line[2:].split(' ')
                curr_cache_hit = int(pairs[1].split(':')[1])
                # Log the count, then the rate, both could be useful

                cache_hit_sample['value'] = curr_cache_hit
                log_sample(file_id, cache_desc_count, {'interface': dp_names[-1], 'action': 'hit'}, cache_hit_sample)

                cache_hit_sample['value'] = (curr_cache_hit - prev_cache_hit) / time_delta
                prev_cache_hit = curr_cache_hit
                log_sample(file_id, cache_desc_rate, {'interface': dp_names[-1], 'action': 'hit'}, cache_hit_sample)

            elif 'port 0' in line:
                # Use port 0 as token to remove the datapath name from the print
                # string in case there are multiple datapaths
                dp_names.pop()

            else:
                # don't care, keep going
                continue


    finish_samples()

def upcall_stats(filename: str):
    print("ovs-post-process-upcall-stats")
    upcall_flow_rate = {'source': 'ovs', 'type': 'upcall-flow-rate', 'class': 'count'}
    upcall_flow_count = {'source': 'ovs', 'type': 'upcall-flow-count', 'class': 'count'}
    upcall_avg_rate = {'source': 'ovs', 'type': 'upcall-flow-avg-rate', 'class': 'count'}
    upcall_avg_count = {'source': 'ovs', 'type': 'upcall-flow-avg-count', 'class': 'count'}
    upcall_max_rate = {'source': 'ovs', 'type': 'upcall-flow-max-rate', 'class': 'count'}
    upcall_max_count = {'source': 'ovs', 'type': 'upcall-flow-max-count', 'class': 'count'}
    upcall_limit_rate = {'source': 'ovs', 'type': 'upcall-flow-limit-rate', 'class': 'count'}
    upcall_limit_count = {'source': 'ovs', 'type': 'upcall-flow-limit-count', 'class': 'count'}
    upcall_duration_rate = {'source': 'ovs', 'type': 'upcall-flow-dump-duration-ms-rate', 'class': 'count'}
    upcall_duration_count = {'source': 'ovs', 'type': 'upcall-flow-dump-duration-ms-count', 'class': 'count'}
    file_id = 'upcall-datapath-stats'
    dp_names = []
    prev_stamp = 0
    curr_stamp = 0
    time_delta = 0
    flow_sample = {'end': 0,'value':0}
    prev_flow = 0
    avg_sample = {'end': 0,'value':0}
    prev_avg = 0
    max_sample = {'end': 0,'value':0}
    prev_max = 0
    limit_sample = {'end': 0,'value':0}
    prev_limit = 0
    duration_sample = {'end': 0,'value':0}
    prev_duration = 0
    with lzma.open(filename,'rt') as file:
        for line in file:
            if 'DATE' in line:
                prev_stamp = curr_stamp
                s = float(line.split(':')[1])
                curr_stamp = int(s * 1000)
                time_delta = curr_stamp - prev_stamp
                flow_sample['end'] = curr_stamp
                avg_sample['end'] = curr_stamp
                max_sample['end'] = curr_stamp
                limit_sample['end'] = curr_stamp
                duration_sample['end'] = curr_stamp
            elif '@' in line:
            # We have found a datapath
                name = line.split(':')[0]
                if name not in dp_names:
                    dp_names.append(name)

            elif 'flows' in line:
                #  flows         : (current 538) (avg 548) (max 747) (limit 200000)
                pairs = line.split(':')[1].strip()
                pairs = pairs.split(') (')
                curr_flow = int(pairs[0].split(' ')[1])
                curr_avg = int(pairs[1].split(' ')[1])
                curr_max = int(pairs[2].split(' ')[1])
                # remove trailing close paren on numeric limit value
                curr_limit = int(pairs[3].split(' ')[1].strip(')'))

                flow_sample['value'] = curr_flow
                avg_sample['value'] = curr_avg
                max_sample['value'] = curr_max
                limit_sample['value'] = curr_limit

                log_sample(file_id, upcall_flow_count,{'interface':dp_names[-1],'counter':'flow'},flow_sample)
                log_sample(file_id, upcall_avg_count,{'interface':dp_names[-1],'counter':'avg'},avg_sample)
                log_sample(file_id, upcall_max_count,{'interface':dp_names[-1],'counter':'max'},max_sample)
                log_sample(file_id, upcall_limit_count,{'interface':dp_names[-1],'counter':'limit'},limit_sample)

                flow_sample['value'] = (curr_flow - prev_flow) / time_delta
                avg_sample['value'] = (curr_avg - prev_avg) / time_delta
                max_sample['value'] = (curr_max - prev_max) / time_delta
                limit_sample['value'] = (curr_limit - prev_limit) / time_delta

                log_sample(file_id, upcall_flow_rate,{'interface':dp_names[-1],'counter':'flow'},flow_sample)
                log_sample(file_id, upcall_avg_rate,{'interface':dp_names[-1],'counter':'avg'},avg_sample)
                log_sample(file_id, upcall_max_rate,{'interface':dp_names[-1],'counter':'max'},max_sample)
                log_sample(file_id, upcall_limit_rate,{'interface':dp_names[-1],'counter':'limit'},limit_sample)

                prev_flow = curr_flow
                prev_avg = curr_avg
                prev_max = curr_max
                prev_limit = curr_limit

            elif 'duration' in line:
                #  dump duration : 10ms
                pairs = line.split(':')[1].strip()
                # slice 'ms' off the value
                curr_duration = int(pairs[:-2])
                duration_sample['value'] = curr_duration
                log_sample(file_id, upcall_duration_count,{'interface':dp_names[-1],'counter':'duration'},duration_sample)

                duration_sample['value'] = (curr_duration - prev_duration) / time_delta
                log_sample(file_id, upcall_duration_rate,{'interface':dp_names[-1],'counter':'duration'},duration_sample)
                prev_duration = curr_duration


            elif 'ufid' in line:
                # use ufid as a per-datapath token
                dp_names.pop()

            else:
                continue

    finish_samples()

def main():
    print("ovs-post-process-multiprocess-launcher")
    conntrack_process = Process(target=conntrack_post_process, args=('conntrack-stats-show.txt.xz',), name='conntrack')
    conntrack_process.start()

    mem_show_process = Process(target=dpctl_memory_show_process, args=('dpctl-memory-show.txt.xz',), name='dpctl-show')
    mem_show_process.start()

    port_counters_process = Process(target=ofctl_port_counters, args=('ofctl-dump-ports.txt.xz',), name='ofctl-dump-ports')
    port_counters_process.start()

    dpctl_stats_process = Process(target=dpctl_datapath_stats, args=('dpctl-datapath-stats.txt.xz',), name='dpctl-dp-stats')
    dpctl_stats_process.start()

    upcall_stats_process = Process(target=upcall_stats, args=('upcall-stats.txt.xz',), name='upcall-stats')
    upcall_stats_process.start()

    # Always join last
    conntrack_process.join()
    mem_show_process.join()
    port_counters_process.join()
    dpctl_stats_process.join()
    upcall_stats_process.join()


if __name__ == '__main__':
    exit(main())
